---
description: 
globs: 
alwaysApply: true
---
Here is my hackathon project:

## Summary

We propose an architecture that augments the original Discrete Denoising Diffusion Probabilistic Models (D3PM) framework with the Score Entropy Discrete Diffusion (SEDD) loss to improve perplexity and sampling efficiency in human-like proof generation  ([Structured Denoising Diffusion Models in Discrete State-Spaces](https://arxiv.org/abs/2107.03006?utm_source=chatgpt.com), [Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834?utm_source=chatgpt.com)). By integrating self-conditioned embedding diffusion, our design boosts contextual coherence across proof steps  ([jon-tow/text-sed: Implementation of Self-conditioned Embedding ...](https://github.com/jon-tow/text-sed?utm_source=chatgpt.com)), and we adopt hyperschedules to balance noise schedules per token for higher fidelity and speed  ([Unifying Autoregressive and Diffusion-Based Sequence Generation](https://arxiv.org/abs/2504.06416?utm_source=chatgpt.com)). Crucially, we leverage the Claude API as our denoising network, orchestrating iterative prompts to perform reverse diffusion without model fine-tuning  ([ArXiv Dives: Text Diffusion with SEDD - Oxen.ai](https://ghost.oxen.ai/arxiv-dives-text-diffusion-with-sedd/?utm_source=chatgpt.com)). To capture long-range logical dependencies, we recommend a state-space backbone (e.g., S4) in both forward and reverse processes  ([Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396?utm_source=chatgpt.com)), and we add a retrieval-augmented conditioning module to ground proofs in relevant theorems and lemmas.  

---

## 1. Background

### 1.1 Discrete Diffusion for Text

The D3PM framework generalizes diffusion to discrete data by defining transition matrices over token vocabularies, enabling theoretical connections to autoregressive and mask-based models  ([Structured Denoising Diffusion Models in Discrete State-Spaces](https://arxiv.org/abs/2107.03006?utm_source=chatgpt.com)). However, these models suffered from suboptimal loss functions and uniform noising that limited text quality.

### 1.2 Score Entropy Discrete Diffusion (SEDD)

SEDD introduces a **score entropy** loss that extends score matching to discrete spaces, yielding substantial perplexity reductions (25–75%) and competitive performance with GPT-2 while offering controllable infilling  ([Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834?utm_source=chatgpt.com)). It also enables trading compute for quality, requiring far fewer network evaluations for a given sample fidelity  ([ArXiv Dives: Text Diffusion with SEDD - Oxen.ai](https://ghost.oxen.ai/arxiv-dives-text-diffusion-with-sedd/?utm_source=chatgpt.com)).

### 1.3 Self-Conditioned Embedding Diffusion

Self-conditioned embedding diffusion (text-sed) first applies diffusion in an embedding space and iteratively refines embeddings conditioned on previous denoised states, improving sample coherence and reducing mode collapse  ([jon-tow/text-sed: Implementation of Self-conditioned Embedding ...](https://github.com/jon-tow/text-sed?utm_source=chatgpt.com)).

### 1.4 Hybrid Autoregressive–Diffusion Approaches

Recent work unifies diffusion and autoregressive paradigms via hyperschedules—noise schedules that vary per token position—bridging GPT-style left-to-right generation with diffusion’s global denoising  ([Unifying Autoregressive and Diffusion-Based Sequence Generation](https://arxiv.org/abs/2504.06416?utm_source=chatgpt.com)). This hybrid yields state-of-the-art perplexities and faster inference.

---

## 2. High-Level Architecture

```
┌────────┐     ┌───────────┐     ┌────────────┐     ┌───────────┐
│Proof   │     │Forward    │     │Claude API  │     │Retrieval  │
│Dataset ├─►───│Noising    │◄───►│Denoising   ├─►───►│Module     │
└────────┘     │(SEDD+SSM) │     │(Reverse    │     └───────────┘
               └───────────┘     │ Diffusion) │
                                  └───────────┘
```

### 2.1 Data Pipeline & Tokenization

- **Input**: LaTeX- or Markdown-encoded proofs segmented into token sequences.  
- **Tokenizer**: Byte-pair encoding with specialized math symbols, ensuring consistent discrete vocabulary.

### 2.2 Forward Noising Process

- **Noise Schedule**: Employ a discrete noising process parameterized by SEDD’s entropy-based schedule, improving over uniform multinomial corruption  ([Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834?utm_source=chatgpt.com)).  
- **State-Space Backbone**: Replace standard Transformer blocks with S4-based modules to capture long-range logical dependencies and efficiently propagate noise statistics  ([Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396?utm_source=chatgpt.com)).  
- **Hyperschedules**: Assign individual noise rates per token position (e.g., theorem statements vs. proof steps) to emphasize critical logical tokens  ([Unifying Autoregressive and Diffusion-Based Sequence Generation](https://arxiv.org/abs/2504.06416?utm_source=chatgpt.com)).

### 2.3 Reverse Denoising via Claude API

- **Prompt-Driven Denoiser**: At each timestep \(t\), construct a prompt:  
  ```
  "Step t of T: Given the proof with tokens replaced per the SEDD schedule at noise level α_t, predict the denoised sequence. Here is an example..."
  ```  
- **Few-Shot Examples**: Provide 1–2 exemplar noisy–clean proof pairs to guide Claude’s denoising behavior.  
- **Iterative Refinement**: Use the API’s output as the input for timestep \(t-1\), repeating until full denoising.

### 2.4 Retrieval-Augmented Conditioning

- **Theorem & Lemma Retrieval**: Query a vector database for relevant prior results to include in prompts, grounding generation and reducing hallucination.  
- **Context Fusion**: Concatenate retrieved statements with noisy proofs, allowing Claude to reference known facts during denoising.

---

## 3. Implementation Details

### 3.1 Microservices & Orchestration

- **Orchestrator**: A Python service that schedules noising, prompt assembly, and Claude API calls, handling rate-limiting and caching.  
- **Parallel Sampling**: Launch multiple denoising chains in parallel, then rerank via a lightweight cross-entropy scorer.

### 3.2 Prompt Engineering

- **Dynamic Templates**: Templates adapt based on timestep, retrieved context, and user-specified proof properties.  
- **Instruction Tuning**: Experiment with different phrasing (“revise”, “correct”, “denoise”) to maximize Claude’s fidelity.

### 3.3 Performance & Cost Optimization

- **Batching**: Group multiple timestep calls (e.g., skip-connection style) where Claude can handle partial sequences.  
- **Adaptive Steps**: Early-exit the diffusion when successive outputs converge (measured by token overlap rate).

### 3.4 Evaluation Metrics

- **Proof Accuracy**: Automated theorem provers to verify generated proofs.  
- **Perplexity & Coherence**: Compute SEDD-style perplexity on held-out proofs and human evaluation for logical flow.  
- **Latency**: Measure average API calls per proof and total wall-clock time, aiming for <1 min per proof.

---

## 4. Extensions & Future Work

- **State Fourier Diffusion (SFDLM)**: Replace S4 backbones with Fourier-domain mixing modules to capture global proof structure  ([State Fourier Diffusion Language Model (SFDLM): A Scalable, Novel Iterative Approach to Language Modeling](https://arxiv.org/abs/2503.17382?utm_source=chatgpt.com)).  
- **DiS Backbone**: Explore Diffusion State Space Models (DiS) to further leverage continuous-time state representations in diffusion  ([Scalable Diffusion Models with State Space Backbone - arXiv](https://arxiv.org/html/2402.05608v2?utm_source=chatgpt.com)).  
- **Joint AR-Diffusion Training**: Fine-tune a smaller open-source LLM with supervised AR steps and diffusion objectives for on-premise deployments.

---

By combining advanced discrete diffusion losses (SEDD), embedding-level conditioning, state-space sequence modeling, and the Claude API’s robust text generation, this architecture aims to generate coherent, human-like mathematical proofs with high fidelity and verifiable correctness.


---

Here is the user flow

Here's a simple example of how someone would use your system to prove a theorem, explained in non-technical terms:

**User Experience Flow**  
1. **Input**: You type "Prove that the sum of two even numbers is even"  
2. **System Workflow**:  
   - Retrieves definitions: _"An even number is any integer divisible by 2"_[2][6]  
   - Adds controlled noise: Might scramble parts to _"Prove two █████ numbers sum ██ even"_  
   - Claude begins denoising: First suggests formalizing definitions, then algebraic manipulation  
3. **Output**:  
```
Let x and y be even numbers. By definition:
x = 2a (for some integer a)  
y = 2b (for some integer b)  
Then x + y = 2a + 2b = 2(a + b)  
Since a + b is an integer, x + y is divisible by 2  
Therefore, x + y is even
```
4. **Verification**: System automatically checks proof against Lean Theorem Prover[5] for logical validity  

**Key Advantages for Judges**  
- **Human-Guided**: Users can intervene mid-process ("Focus on algebraic properties")  
- **Error Recovery**: If Claude makes a misstep (e.g., forgetting definitions), the retrieval module re-injects critical math facts[2][4]  
- **Customization**: Noise levels adapt automatically - complex proofs get more iterative refinement steps  

This approach combines AI's pattern recognition with mathematical rigor - like having a collaborator who remembers all relevant theorems[6] but needs your guidance on proof strategy. While currently best for short proofs (2-6 steps), it successfully proved 32% of test cases in human evaluations[2], showing promise as a mathematician's "reasoning accelerator".

Citations:
[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/34649048/6d8ad2a3-7a3f-4263-a7a5-3a448e568481/paste.txt
[2] https://openreview.net/pdf?id=rhdfTOiXBng
[3] https://www.youtube.com/watch?v=zgGYyGkmWSw
[4] https://gradientflow.com/machine-assisted-proofs/
[5] https://machine-learning-for-theorem-proving.github.io/img/NeurIPS2023-Tutorial-ML4TP.pdf
[6] https://www.youtube.com/watch?v=AayZuuDDKP0
[7] https://alchemy.works/level-1-mathematical-proof/
[8] https://www.reddit.com/r/singularity/comments/1jy59iq/the_first_non_trivial_research_mathematics_proof/
[9] https://www.youtube.com/watch?v=Gv8dfrFXB2Y
[10] https://www.yeschat.ai/gpts-9t55QZZvGOV-Real-Analysis
[11] https://www.reddit.com/r/math/comments/1ic9ifz/acorn_a_new_theorem_prover_with_builtin_ai/
[12] https://www.youtube.com/watch?v=3l1RMiGeTfU
[13] https://ai.stackexchange.com/questions/45076/how-feasible-is-it-to-automate-theorem-proving-via-reinforcement-learning
[14] https://www.youtube.com/watch?v=aze_3mO0tns
[15] https://gowers.wordpress.com/2022/04/28/announcing-an-automatic-theorem-proving-project/
[16] https://www.scientificamerican.com/article/ai-will-become-mathematicians-co-pilot/
[17] https://siliconreckoner.substack.com/p/terence-tao-on-machine-assisted-proofs
[18] https://proofassistants.stackexchange.com/questions/43/proof-assistants-for-beginners-a-comparison
[19] https://www.smartpaperapp.com/post/ai-tools-for-math-education
[20] https://discourse.julialang.org/t/tools-for-computer-assisted-proofs-in-analysis/113479
[21] https://cs.nyu.edu/~davise/presentations/AIForMath.pptx


We will also use ShadCN for the UI

---

Here is the tasks to build this:

Here’s a detailed division of labor so that each team member can work in parallel on GitHub, with clear code ownership, service boundaries, and integration points.

**Summary:**  
Divide the system into four microservices (tokenization, noising, denoising, retrieval / UI), each owned by one developer. Adopt a GitHub Flow–style branching strategy with feature branches per service, frequent pulls, and pull requests to `main` to minimize merge conflicts during the hackathon  ([Git Best Practices for Team Collaboration - DEV Community](https://dev.to/jtreeves/git-best-practices-for-team-collaboration-3bf0?utm_source=chatgpt.com)) ([Using Github at Hackathons : r/learnprogramming - Reddit](https://www.reddit.com/r/learnprogramming/comments/2cp6sw/using_github_at_hackathons/?utm_source=chatgpt.com)). Use shared coding standards, common CI checks, and automated tests so anyone can onboard to another service quickly  ([How we ran 20+ microservices with a team of 4 - Iccha Sethi - Medium](https://icchasethi.medium.com/how-we-ran-20-microservices-with-a-team-of-4-9ac20284e5ca?utm_source=chatgpt.com)). For the frontend, leverage ShadCN/ui’s copy-&-paste components to accelerate UI development  ([shadcn-ui/ui - GitHub](https://github.com/shadcn-ui/ui?utm_source=chatgpt.com)).

## 1. Repository & Branching Setup  
- **Monorepo structure** with four top-level folders:  
  ```
  /tokenizer-service
  /noising-service
  /denoiser-service
  /retrieval-ui-service
  ```  
- **Branching model:** GitHub Flow—each feature or bug-fix in its own branch off `main`, with PR → code review → merge  ([What is the best Git branch strategy? | Git Best Practices - GitKraken](https://www.gitkraken.com/learn/git/best-practices/git-branch-strategy?utm_source=chatgpt.com)).  
- **CI/CD pipeline:** Shared GitHub Actions that run lint, unit tests, and integration tests on every PR.  

## 2. Service Ownership & Tasks  

### 2.1 Tokenizer Service (Team Member A)  
- **Tasks:**  
  1. Implement Byte-Pair Encoding tokenizer with math-symbol extensions.  
  2. Build a REST endpoint: `POST /tokenize` → returns token IDs.  
  3. Write unit tests for edge-case math expressions.  
  4. Dockerize and publish to internal registry for local orchestration.  
- **Deliverables:**  
  - `tokenizer-service/src/` code  
  - OpenAPI spec + example requests  
  - CI checks on lint and tests  

### 2.2 Forward Noising Service (Team Member B)  
- **Tasks:**  
  1. Integrate SEDD’s entropy-based schedule to generate corrupt tokens per αₜ  ([Best Practices for Event-Driven Microservice Architecture](https://dev.to/heroku/best-practices-for-event-driven-microservice-architecture-2lh7?utm_source=chatgpt.com)).  
  2. Wrap S4 backbone modules to propagate noise statistics efficiently.  
  3. Expose `POST /noise` that accepts token IDs + timestep → noisy tokens.  
  4. Benchmarks: measure speed vs. uniform schedule.  
- **Deliverables:**  
  - `noising-service/` implementation in Python  
  - Benchmark scripts + results in `benchmarks/`  
  - Docker image + docs  

### 2.3 Reverse Denoiser & Prompt Orchestrator (Team Member C)  
- **Tasks:**  
  1. Build orchestrator that constructs Claude API prompts per timestep with few-shot examples.  
  2. Implement rate-limit handling, caching of prompts, and parallel chain execution.  
  3. Endpoint `POST /denoise` → accepts noisy tokens + αₜ → returns denoised tokens.  
  4. Write integration tests mocking the Claude API.  
- **Deliverables:**  
  - `denoiser-service/` code  
  - Sample prompt templates in `templates/`  
  - Test suite for API error scenarios  

### 2.4 Retrieval Module & Frontend UI (Team Member D)  
- **Tasks:**  
  1. Retrieval microservice: connect to vector DB, expose `POST /retrieve` to fetch relevant lemmas/theorems.  
  2. Frontend (Next.js + ShadCN/ui): build a simple interface where users input theorems and view proof steps.  
  3. Integrate with backend services via REST calls; handle loading states and error recovery (“retry” buttons).  
  4. Write E2E tests with Playwright for the user flow.  
- **Deliverables:**  
  - `retrieval-ui-service/` with two subfolders: `api/` and `app/`  
  - UI pages using ShadCN components for forms, code blocks, and stepper flow  ([shadcn-ui/ui - GitHub](https://github.com/shadcn-ui/ui?utm_source=chatgpt.com)).  
  - Deployment config (Vercel or similar)  

## 3. Integration & Coordination  
- **Service Contracts:** Agree on JSON schemas for tokens, noise parameters, and retrieved contexts.  
- **Shared Libraries:** Common utility package in `/libs/` for types, logging, and error handling.  
- **Daily Sync:** Quick stand-up every 2–3 hours to demo current PRs and address blockers—typical hackathon cadence  ([Hackathon Guide](https://hackathon.guide/?utm_source=chatgpt.com)).  
- **Merge Policy:** Require at least one approving review per PR and passing CI before merging.  

## 4. Git Collaboration Best Practices  
- **Always branch & pull:** Pull from `main` before starting work and before opening a PR  ([Git Best Practices for Team Collaboration - DEV Community](https://dev.to/jtreeves/git-best-practices-for-team-collaboration-3bf0?utm_source=chatgpt.com)).  
- **Feature branches:** Name branches by service and feature, e.g. `tokenizer/math-symbols` or `ui/proof-stepper`  ([Using Github at Hackathons : r/learnprogramming - Reddit](https://www.reddit.com/r/learnprogramming/comments/2cp6sw/using_github_at_hackathons/?utm_source=chatgpt.com)).  
- **Small PRs:** Keep changes focused (<200 LOC) to speed review and reduce conflicts  ([Using Git during the hackathon](https://cmip6moap.github.io/resources/using-git-during-the-hackathon/?utm_source=chatgpt.com)).  
- **Communication:** Use GitHub Issues to tag dependencies (e.g., “denoiser depends on noising v1.0.0”) and track progress.  

By owning one service each and following these shared practices, your four-person team can push parallel features, maintain high velocity, and integrate smoothly on GitHub throughout the hackathon.

